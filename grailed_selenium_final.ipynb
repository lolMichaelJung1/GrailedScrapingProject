{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium Web Scraper - Grailed.com\n",
    "\n",
    "### About Grailed.com:\n",
    "\n",
    "Grailed is an online community driven marketplace where individuals can buy and sell clothes. Sellers can upload images and descriptions of their items whereas buyers can select which brand or designer they want to browse through. \n",
    "\n",
    "### About Selenium\n",
    "\n",
    "Selenium is a headless browser, which means it enables users to mock human-browsing behavior. Text can be entered in search boxes, buttons can be clicked, and new tabs can be created. It's super fun! More information can be found in the following: [Selenium docs](http://selenium-python.readthedocs.io/getting-started.html), [locating elements](http://selenium-python.readthedocs.io/locating-elements.html#locating-elements), and [FAQs](http://selenium-python.readthedocs.io/faq.html). \n",
    "\n",
    "\n",
    "### Objective:\n",
    "\n",
    "Personally, I spend a bit too much time on grailed. I am also a bit lazy, so I wanted to create a webscraper that will automatically return the brand, name, picture, size, and price of clothes that I like!\n",
    "\n",
    "I have created 2 functions.\n",
    "1. `grailed_scraper()` scrapes and filters items from grailed.com and returns the scraped information in a dataframe\n",
    "2. `photo_download_displayer()` downloads each item's picture and displays each item's picture, price, and name in the Jupyter Notebook\n",
    "\n",
    "More detailed information is located below:\n",
    "\n",
    "#### Import Modules\n",
    "The following modules are imported. Pandas is needed for constructing a dataframe, Numpy is used to randomly select numbers, selenium is needed for interacting with the webpage and scraping information, and HTML related modules are needed as well to filter and grab necessary information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function #1\n",
    "The objective is to scrape and filter items from grailed.com.\n",
    "The function `grailed_scraper()` receives 3 inputs.\n",
    "1. `search_text` - Allows us to input whatever query into the search box. For example, I can look up the brand 'Online Ceramics' (which you will see repeatedly in this example)\n",
    "![grailed search box](../pics/GRAILED_SEARCH_BOX.jpeg)\n",
    "\n",
    "\n",
    "2. `category` - Filter for what kind of article of clothing we want, such as Tops, Bottoms, or Shoes.\n",
    "![grailed search box](../pics/GRAILED_CATEGORY.jpeg)\n",
    "\n",
    "\n",
    "3. `size` - Filter for sizes, ie small/medium/large for Tops, size 30 for Bottoms, and size 10.5 for shoes\n",
    "<img src=\"../pics/GRAILED_SIZE.jpeg\" alt=\"drawing\" width=\"250\"/>\n",
    "\n",
    "\n",
    "After taking in 3 inputs, the `grailed_scraper()` function will open up a separate chrome driver, fill in the search box, click on the categories and the sizes, scrape all of the necessary information of each item, such as its name, price, url, picture url, and posting date, and then neatly return a dataframe with the information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from IPython.display import Image, display, HTML  \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "def grailed_scraper(search_text, category = 'Tops', size = 'medium'):  \n",
    "\n",
    "    # Connect to the chrome driver\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--start-maximized\")  # Using full screen\n",
    "    driver = webdriver.Chrome(service=Service(executable_path=\"chromedriver.exe\"), options=chrome_options)\n",
    "\n",
    "    # Open new tab. We are going straight to grailed.com\n",
    "    driver.get('https://www.grailed.com/')\n",
    "    \n",
    "    # Expand chrome window\n",
    "    # driver.maximize_window()\n",
    "    \n",
    "    # Wait one second\n",
    "    sleep(1)\n",
    "\n",
    "    # Search whatever brand we want in the search text box\n",
    "# Search whatever brand we want in the search text box\n",
    "    driver.find_element(\"id\", \"header_search-input\").send_keys(search_text)\n",
    "    sleep(1)\n",
    "    \n",
    "    # Press the enter key\n",
    "    driver.find_element(\"id\", \"header_search-input\").send_keys(u'\\ue007', Keys.ENTER)\n",
    "    elem = driver.find_element(By.XPATH, \"//div[@class='UsersAuthentication']\")  # User login window will pop up\n",
    "    ac = ActionChains(driver)\n",
    "    ac.move_to_element(elem).move_by_offset(250, 0).click().perform()  # Clicking away from the login window\n",
    "    driver.implicitly_wait(1)\n",
    "    element = driver.find_elements(By.XPATH, \"//*[contains(text(),'Show Only')]\")  # Finding the show-only button again\n",
    "    element[0].click()  # Clicking now that the pop-up is away\n",
    "    # Clicks on sizes, then tops, then sizes!\n",
    "    sleep(5)\n",
    "    \n",
    "    '''\n",
    "    Category and size filtering process:\n",
    "    If the category is equal to Tops, then the function will examine the following:\n",
    "    If there are both tops and outerwear in the categories list, it will return the size for both categories.\n",
    "    If there is only outerwear in the categories list, it will return the size for outerwear.\n",
    "    If there is only tops in the categories list, it will return the size for tops.\n",
    "    The nested while loops are used to tell selenium to continue to click on the right buttons until it works correctly\n",
    "    '''\n",
    "    \n",
    "    if category == 'Tops':\n",
    "        \n",
    "    # Size Filter\n",
    "        size_dict = {\n",
    "                     'small'  : 'S/44-46',\n",
    "                     'medium' : 'M/48-50',\n",
    "                     'large'  : 'L/52-54'\n",
    "                    }\n",
    "        \n",
    "        selected_size = size_dict[size]\n",
    "\n",
    "\n",
    "        # Grab HTML page source\n",
    "        html = driver.page_source\n",
    "        html = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        categories_list = [category.text for category in list(set(html.find_all('h3')))]\n",
    "\n",
    "        # if outerwear and tops are part of the cateogories, filter for both 'tops' and 'outerwear'\n",
    "        if ('Outerwear' in categories_list) and ('Tops' in categories_list):\n",
    "            while True:\n",
    "                try:\n",
    "                    driver.find_element(By.XPATH, \"//h3[contains(text(), 'Tops')]\").click()\n",
    "                    sleep(1)\n",
    "                    try:\n",
    "                        driver.find_element(By.XPATH, f\"//input[@type='checkbox' and @name = '{selected_size}' and @value = '{selected_size}']\").click()\n",
    "                        sleep(1)\n",
    "                    except:\n",
    "                        print('Different Sizes')\n",
    "                        break\n",
    "                    driver.find_element(By.XPATH,\"//h3[contains(text(), 'Outerwear')]\").click()\n",
    "                    sleep(1)\n",
    "                    try:\n",
    "                        driver.find_elements(By.XPATH, f\"//*[contains(text(), '{selected_size}')]\")[1].click()\n",
    "                    except:\n",
    "                        print('Different Sizes')\n",
    "                        break\n",
    "                except:\n",
    "                     continue\n",
    "                else:\n",
    "                     break\n",
    "        elif ('Outerwear' not in categories_list) and ('Tops' in categories_list):\n",
    "            while True:\n",
    "                try:\n",
    "                    driver.find_element(By.XPATH, \"//h3[contains(text(), 'Tops')]\").click()\n",
    "                    sleep(1)\n",
    "                    driver.find_element(By.XPATH, f\"//input[@type='checkbox' and @name = '{selected_size}' and @value = '{selected_size}']\").click()\n",
    "                    sleep(1)\n",
    "                except:\n",
    "                     continue\n",
    "                else:\n",
    "                     break\n",
    "        elif ('Outerwear' in categories_list) and ('Tops' not in categories_list):\n",
    "            while True:\n",
    "                try:\n",
    "                    driver.find_element(By.XPATH, \"//h3[contains(text(), 'Outerwear')]\").click()\n",
    "                    sleep(1)\n",
    "                    driver.find_element(By.XPATH, f\"//input[@type='checkbox' and @name = '{selected_size}' and @value = '{selected_size}']\").click()\n",
    "                    sleep(1)\n",
    "                except:\n",
    "                     continue\n",
    "                else:\n",
    "                     break\n",
    "    else: \n",
    "        while True:\n",
    "                try:\n",
    "                    driver.find_element(By.XPATH, f\"//h3[contains(text(), '{category}')]\").click()\n",
    "                    sleep(1)\n",
    "                    driver.find_element(By.XPATH, f\"//input[@type='checkbox' and @name = '{size}' and @value = '{size}']\").click()\n",
    "                    sleep(1)\n",
    "                except:\n",
    "                     continue\n",
    "                else:\n",
    "                     break\n",
    "    \n",
    "\n",
    "    # Scroll all the way down to the page (because more items load as you go down)\n",
    "    # https://stackoverflow.com/questions/20986631/how-can-i-scroll-a-web-page-using-selenium-webdriver-in-python\n",
    "    SCROLL_PAUSE_TIME = 1\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Wait one second.\n",
    "    sleep(1)\n",
    "\n",
    "    # Grab the page source.\n",
    "\n",
    "    html = driver.page_source\n",
    "\n",
    "    # Beautiful Soup it!\n",
    "    html = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "\n",
    "    # finding all posts on page\n",
    "    all_posts = html.find_all('p', {'class' : \"listing-size sub-title\"})\n",
    "\n",
    "    # total number of postings \n",
    "    num_postings = len(all_posts)\n",
    "\n",
    "    posts_to_keep = list(range(num_postings))\n",
    "\n",
    "    # Print how many postings there are\n",
    "    print(f'Number of Items: {num_postings}')\n",
    "\n",
    "    # loop through the items to grab all of the information\n",
    "\n",
    "    urls = []\n",
    "    names = []\n",
    "    prices = []\n",
    "    bump_date = []\n",
    "    post_date = []\n",
    "    pics = []\n",
    "\n",
    "    for position in posts_to_keep:\n",
    "\n",
    "        url = 'https://www.grailed.com' + html.find_all('div', {'class' : 'feed-item'})[position].find('a')['href']\n",
    "        urls.append(url)\n",
    "\n",
    "        name = html.find_all('div', {'class' : \"truncate\"})[position].text\n",
    "        names.append(name)\n",
    "\n",
    "        price = html.find_all('div', {'class' : \"listing-price\"})[position].text.split('$')[1].replace(',', \"\")\n",
    "        prices.append(int(price))\n",
    "\n",
    "        all_date_info = html.find_all('div', {'class' : 'feed-item'})[position].find_all('p')[0].text.split('(')\n",
    "\n",
    "        # removing unwanted text such as 'about', 'ago', ')'\n",
    "        all_date_info = [item.replace('\\xa0ago', '').replace(')', '').replace('about ', '') for item in all_date_info]\n",
    "\n",
    "        if len(all_date_info) == 2:\n",
    "            bump_date.append(all_date_info[0])\n",
    "            post_date.append(all_date_info[1])\n",
    "            \n",
    "        else: # meaning there is only one timestamp, so bump date = posted date\n",
    "            bump_date.append(all_date_info[0])\n",
    "            post_date.append(all_date_info[0])\n",
    "\n",
    "        # Grab each item's picture url. Some items don't have them because they are lazy loaded :( \n",
    "        try:\n",
    "            pic_url = html.find_all('div', {'class' : \"listing-cover-photo\"})[position].find('img')['src']\n",
    "            pics.append(pic_url)\n",
    "        except:\n",
    "            pics.append('LazyLoader')\n",
    "\n",
    "\n",
    "    clothes_dict = {'name' : names,\n",
    "                    'bump_date' : bump_date,\n",
    "                    'post_date' : post_date,\n",
    "                    'price_dollar' : prices,\n",
    "                    'url' : urls,\n",
    "                    'pic_url' : pics\n",
    "                   }\n",
    "\n",
    "    df = pd.DataFrame(clothes_dict)\n",
    "    df.head()\n",
    "\n",
    "    # Sort dataframe based on how expensive the items are\n",
    "    df = df.sort_values('price_dollar')\n",
    "    df.reset_index(drop = True, inplace= True)\n",
    "    \n",
    "    # Close the driver, you are done!\n",
    "    driver.close()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function #1: Example\n",
    "Let's use the brand [Online Ceramics](https://online-ceramics.com/) as an example! I am a fan of their medium sized t-shirts and hoodies ðŸ™‚ðŸ™‚\n",
    "\n",
    "**Note:** I can be more specific with my search query. For example, I can choose 'Online Ceramics white T-Shirts' for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//div[@class='UsersAuthentication']\"}\n  (Session info: chrome=123.0.6312.124); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x00007FF712027032+63090]\n\t(No symbol) [0x00007FF711F92C82]\n\t(No symbol) [0x00007FF711E2EC65]\n\t(No symbol) [0x00007FF711E7499D]\n\t(No symbol) [0x00007FF711E74ADC]\n\t(No symbol) [0x00007FF711EB5B37]\n\t(No symbol) [0x00007FF711E9701F]\n\t(No symbol) [0x00007FF711EB3412]\n\t(No symbol) [0x00007FF711E96D83]\n\t(No symbol) [0x00007FF711E683A8]\n\t(No symbol) [0x00007FF711E69441]\n\tGetHandleVerifier [0x00007FF7124225AD+4238317]\n\tGetHandleVerifier [0x00007FF71245F70D+4488525]\n\tGetHandleVerifier [0x00007FF7124579EF+4456495]\n\tGetHandleVerifier [0x00007FF712100576+953270]\n\t(No symbol) [0x00007FF711F9E54F]\n\t(No symbol) [0x00007FF711F99224]\n\t(No symbol) [0x00007FF711F9935B]\n\t(No symbol) [0x00007FF711F89B94]\n\tBaseThreadInitThunk [0x00007FF985257344+20]\n\tRtlUserThreadStart [0x00007FF985B026B1+33]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rick_owens \u001b[38;5;241m=\u001b[39m \u001b[43mgrailed_scraper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRick Owens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTops\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmedium\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mgrailed_scraper\u001b[1;34m(search_text, category, size)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Press the enter key\u001b[39;00m\n\u001b[0;32m     36\u001b[0m driver\u001b[38;5;241m.\u001b[39mfind_element(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader_search-input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msend_keys(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\ue007\u001b[39;00m\u001b[38;5;124m'\u001b[39m, Keys\u001b[38;5;241m.\u001b[39mENTER)\n\u001b[1;32m---> 37\u001b[0m elem \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m//div[@class=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUsersAuthentication\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# User login window will pop up\u001b[39;00m\n\u001b[0;32m     38\u001b[0m ac \u001b[38;5;241m=\u001b[39m ActionChains(driver)\n\u001b[0;32m     39\u001b[0m ac\u001b[38;5;241m.\u001b[39mmove_to_element(elem)\u001b[38;5;241m.\u001b[39mmove_by_offset(\u001b[38;5;241m250\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mclick()\u001b[38;5;241m.\u001b[39mperform()  \u001b[38;5;66;03m# Clicking away from the login window\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\18188\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:742\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    739\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    740\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\18188\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:348\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    346\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 348\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\18188\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//div[@class='UsersAuthentication']\"}\n  (Session info: chrome=123.0.6312.124); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x00007FF712027032+63090]\n\t(No symbol) [0x00007FF711F92C82]\n\t(No symbol) [0x00007FF711E2EC65]\n\t(No symbol) [0x00007FF711E7499D]\n\t(No symbol) [0x00007FF711E74ADC]\n\t(No symbol) [0x00007FF711EB5B37]\n\t(No symbol) [0x00007FF711E9701F]\n\t(No symbol) [0x00007FF711EB3412]\n\t(No symbol) [0x00007FF711E96D83]\n\t(No symbol) [0x00007FF711E683A8]\n\t(No symbol) [0x00007FF711E69441]\n\tGetHandleVerifier [0x00007FF7124225AD+4238317]\n\tGetHandleVerifier [0x00007FF71245F70D+4488525]\n\tGetHandleVerifier [0x00007FF7124579EF+4456495]\n\tGetHandleVerifier [0x00007FF712100576+953270]\n\t(No symbol) [0x00007FF711F9E54F]\n\t(No symbol) [0x00007FF711F99224]\n\t(No symbol) [0x00007FF711F9935B]\n\t(No symbol) [0x00007FF711F89B94]\n\tBaseThreadInitThunk [0x00007FF985257344+20]\n\tRtlUserThreadStart [0x00007FF985B026B1+33]\n"
     ]
    }
   ],
   "source": [
    "rick_owens = grailed_scraper('Rick Owens', category = 'Tops', size='medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like there are 39 medium t-shirts/hoodies from Online Ceramics!\n",
    "\n",
    "Let's examine the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>bump_date</th>\n",
       "      <th>post_date</th>\n",
       "      <th>price_dollar</th>\n",
       "      <th>url</th>\n",
       "      <th>pic_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name, bump_date, post_date, price_dollar, url, pic_url]\n",
       "Index: []"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_ceramics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function #2\n",
    "\n",
    "The objective of `photo_downloader_displayer()` is to download each item's picture and display each item's picture, price, name and URL in the Jupyter Notebook!\n",
    "\n",
    "The function `photo_downloader_displayer()` receives 2 inputs.\n",
    "1. `folder_name` - This name is quite self-explanatory. The function will create a folder with the folder name and store all of the downloaded photos in the folder. \n",
    "\n",
    "\n",
    "2. `df` - The df is the dataframe that was created from `grailed_scraper()`\n",
    "\n",
    "\n",
    "After taking in 2 inputs, the `photo_downloader_displayer()` function will open up a separate chrome driver using each individual item's stored URL in the dataframe. It will then grab and open up a new tab with the item's picture URL. Next, a screenshot will be taken and saved into the created folder. After downloading all of the screenshots, a for-loop will run through the dataframe and display the item's information in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def photo_downloader_displayer(folder_name, df):\n",
    "    \n",
    "    import os \n",
    "    \n",
    "    # create folder\n",
    "    \n",
    "    directory = f'../{folder_name}_pics/'\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)\n",
    "    \n",
    "    # Create a new picture url list\n",
    "    pic_url = []\n",
    "\n",
    "    # Loop through each row in the dataframe\n",
    "    for pos in df.index:\n",
    "        \n",
    "        url = df['url'][pos]\n",
    "\n",
    "        driver = webdriver.Chrome(executable_path=\"../chromedriver/chromedriver\")\n",
    "\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Window name\n",
    "        window_before = driver.window_handles[0]\n",
    "        \n",
    "        # Wait one second.\n",
    "        sleep(2)\n",
    "\n",
    "        html = driver.page_source\n",
    "\n",
    "        # Beautiful Soup it!\n",
    "        html = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        # Grab image_url\n",
    "        image_url = html.find('div', {'class' : '-image-wrapper'}).find('img')['src']\n",
    "\n",
    "        # Store it in pic_url\n",
    "        pic_url.append(image_url)\n",
    "        \n",
    "        \n",
    "        # Open image url in new tab\n",
    "        driver.execute_script(f'''window.open(\"{image_url}\",\"_blank\");''')\n",
    "        \n",
    "        \n",
    "        # switch to image url\n",
    "        window_after = driver.window_handles[1]\n",
    "        \n",
    "        driver.switch_to.window(window_after)\n",
    "        \n",
    "        sleep(np.random.choice(range(1,3)))\n",
    "        \n",
    "        # save the image\n",
    "        driver.save_screenshot(f\"{directory}{str(pos)}_screenshot.png\")\n",
    "        \n",
    "        # Close both tabs\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "    # Reassigning pic url's for the dataframe:\n",
    "    df['pic_url'] = pic_url\n",
    "    \n",
    "    # Displays images and necessary information in the jupyter notebook\n",
    "    for idx in df.index:\n",
    "        display(Image(f\"{directory}{str(idx)}_screenshot.png\"))\n",
    "        print(f\"Name: {df.at[idx, 'name']}\")\n",
    "        print(f\"Price: ${df.at[idx, 'price_dollar']}\")\n",
    "        print(f\"URL: {df.at[idx, 'url']}\")\n",
    "        print('')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function #2: Example\n",
    "No surprises here, we're going to use Online Ceramics as an example already. It's also because I already have the created dataframe from running the first function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "online_ceramics = photo_downloader_displayer('online_ceramics', online_ceramics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ta-Da!\n",
    "\n",
    "There you have it! You can play around Selenium and any other website to mess around with. Based on this function, I'm definitely not going to buy some of these shirts. They're wayyyyy too expensive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
